---
permalink: /about/
title: "About"
author_profile: true
---

Human behaviors and interactions shape most aspects of our lives, constrained by biological functions. My passion lies in quantitatively understanding such intelligence and living systems. On the other hand, in the evolutionary history of biology, powerful regulatory mechanism and control systems in organism have shown remarkable advantages. Can the logic behind this be applied to more generalized living systems? For example, in human societal systems, which are undergoing rapid extension in scale, complexity and productivity, can we use such underlying principles to reduce the costs of revolution? I have long been curious about this. So, I began my research journey in biophysics since my second year of college. My journey has progressed through: 1) theoretical studies based on statistical physics, 2) data-driven analysis based on engineering, and 3) the development of computational methods for connecting data and physical theory using machine learning. 

During the initial stage of my studies, under the guidance of Professor Jin Wang, I focused on using nonequilibrium statistical physics to uncover the quantitative rules behind the brain functions. This perspective offers several advantages. Given the inherent randomness in brain systems, statistical probability evolution captures deterministic components that are valuable for inference and prediction. Additionally, as an open living system, neurons and the brain continuously exchange energy, matter, and information with their environment, which can be quantified. Therefore, I conceptualized the neuron and cortical circuits as systems characterized by nonequilibrium and fluctuations. For single neurons, there are generally two different types of excitation patterns in the mammalian brain, contributing to neural heterogeneity (like different sexes in society). I am curious about the key characteristics at the single-neuron level that play a role in neural computations. To address this, I studied both types of synapses near their thresholds, quantifying the robustness against noise and the entropy flow towards the system under a natural environment. The results demonstrate robust encoding in type I neurons and a noise-induced synchronization phase in type II neurons. 

Guided by a physics-based approach to construction, the bottom level and large-scale systems are often well-suited for abstract modeling and, therefore, are the simplest for analysis. Therefore, after studying single neuron, I shifted my focus to the cortex level, aiming to uncover the global dynamics of cognition emerging from complex cortical interactions. Cognitive processes rely on collaborative activity across the cortex, organized with spatial and temporal order. I wonder how this large-scale activity is influenced by the cortical neural connections and what is the underlying evolutionary effect. Accordingly, I identified the most probable transition path between selective and non-selective states by introducing path integral methods (analogous to the least-action principle). The findings reveal that the temporal order of response times across cortical areas benefits from an asymmetric hierarchical structure, suggesting an adaptive structure optimized for information processing. Moreover, when maintaining identical memory duration, a higher proportion of cross-area connections appears to reduce entropy production, implying lower energy consumption (considering entropy flow as a measure of energy in living systems). Although experimental comparisons of energy are challenging, the evolution of connection distances in mammalian brains appears to align with this effect. My work also provided a framework to estimate energy and predict potential state switching in cognitive tasks. 

But more questions arose. Real systems are complex. When we ignore biological details, analysis becomes easier, but it limits the potential discoveries. Whereas incorporating more biological details requires the integration of experimental data. I shifted my focus to the complexity of real experiments and found that it is not easy to establish a direct relationship between theoretical and experimental results. Therefore, I set out to explore a different research paradigm. At Simon Fraser University, I investigated cerebra-cerebella interactions with MEG technique (a method for recording human neural activity). Here, I learned to analyze data with machine learning tools to examine the characteristics of certain brain activities and disorders. This research felt like what outpatient doctors do when diagnosing patients. It’s interesting, however, I still want to explore how to relate the time series data to the dynamics of cortical interactions behind the phenomenology. 

Later, I returned to Wang’s lab. I expressed my interest in finding such relationship between data-driven approaches and theoretical analysis. Then we focus on developing our method for reconstructing dynamical systems from time series data in neuroscience and predicting the probability evolution of neural systems. I aim to use deep learning approach to build the driving field including deterministic and stochastic components, which fits within the framework on physics. The challenge lies in recognizing the dynamical relevance within the data. When we use simulated data, we exactly know what should be recognized, which allows deep learning neural networks to perform well. However, real data, such as neural signaling, is complicated by uncovered causality and noise. 

Therefore, we must identify the limit of the information contained in the data and work to extend this limit with modern techniques. Revolution is ongoing. With the advancement of techniques, our understanding of the biological systems is different from a “black box”. Yet, as data volume grows explosively, our explanations at certain system levels remain less than fully persuasive. Unlike fields that leverage AI solely to tackle computational complexity, our understanding of living systems remains fundamentally limited. Behind the factor of variation, a “gap” still persists between bottom-up mechanism and phenomenon.

